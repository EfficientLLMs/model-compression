{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/vmasti/efficient-llms-capstone/MoRA/peft-mora\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/vmasti/miniconda3/lib/python3.10/site-packages (from peft==0.9.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vmasti/miniconda3/lib/python3.10/site-packages (from peft==0.9.0) (23.2)\n",
      "Requirement already satisfied: psutil in /home/vmasti/miniconda3/lib/python3.10/site-packages (from peft==0.9.0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/vmasti/miniconda3/lib/python3.10/site-packages (from peft==0.9.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/vmasti/miniconda3/lib/python3.10/site-packages (from peft==0.9.0) (2.2.2)\n",
      "Requirement already satisfied: transformers in /home/vmasti/miniconda3/lib/python3.10/site-packages (from peft==0.9.0) (4.39.3)\n",
      "Requirement already satisfied: tqdm in /home/vmasti/miniconda3/lib/python3.10/site-packages (from peft==0.9.0) (4.65.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/vmasti/miniconda3/lib/python3.10/site-packages (from peft==0.9.0) (0.28.0)\n",
      "Requirement already satisfied: safetensors in /home/vmasti/miniconda3/lib/python3.10/site-packages (from peft==0.9.0) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/vmasti/miniconda3/lib/python3.10/site-packages (from peft==0.9.0) (0.22.2)\n",
      "Requirement already satisfied: filelock in /home/vmasti/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.9.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vmasti/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.9.0) (2024.2.0)\n",
      "Requirement already satisfied: requests in /home/vmasti/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.9.0) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vmasti/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.9.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/vmasti/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.9.0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/vmasti/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.9.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/vmasti/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.9.0) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vmasti/miniconda3/lib/python3.10/site-packages (from transformers->peft==0.9.0) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/vmasti/miniconda3/lib/python3.10/site-packages (from transformers->peft==0.9.0) (0.15.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vmasti/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.9.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vmasti/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vmasti/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vmasti/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vmasti/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.0) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/vmasti/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.9.0) (1.3.0)\n",
      "Building wheels for collected packages: peft\n",
      "  Building editable for peft (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peft: filename=peft-0.9.0-0.editable-py3-none-any.whl size=10336 sha256=9b9a7f282c56bb77292ec227f0b2598caa9e0ca0a49e8997d41ac264370a58d9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-sm7hq077/wheels/d5/54/5c/0b1f54c87eb0a8617c99aebb7fd288f1cd0ef521abb5277f2b\n",
      "Successfully built peft\n",
      "Installing collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.9.0\n",
      "    Uninstalling peft-0.9.0:\n",
      "      Successfully uninstalled peft-0.9.0\n",
      "Successfully installed peft-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ../MoRA/peft-mora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mora_fine_tune import evaluate_model\n",
    "from accelerate import Accelerator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"large_model\": \"EleutherAI/pythia-410m\",\n",
    "    \"small_model\": \"EleutherAI/pythia-70m\",\n",
    "    \"large_adapter\": \"./weight/pythia_410m_r=8_0.0001_fixed\",\n",
    "    \"rank\": 8,\n",
    "    \"mora_type\": 6,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"eval_dataloader\": torch.load(\"./data/eval_dataloader.pt\"),\n",
    "    \"accelerator\": Accelerator(),\n",
    "    # \"large_layers\": 24,\n",
    "    # \"small_layers\": 6,\n",
    "}\n",
    "\n",
    "# large_rank * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args['large_model'],  # standard model; the same tokenizer is used for all models\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the large fine-tuned model\n",
    "\n",
    "large_model = GPTNeoXForCausalLM.from_pretrained(args[\"large_model\"])\n",
    "large_model.load_adapter(args[\"large_adapter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the small model with MoRA weights\n",
    "\n",
    "config = LoraConfig(\n",
    "    # enable MoRA\n",
    "    use_mora=True,\n",
    "    # type 1 (Sharing) for large lora ranks, Eq. 6 in paper\n",
    "    # type 6 (RoPE based) for small lora ranks, Eq. 9 in paper\n",
    "    mora_type=args[\"mora_type\"],\n",
    "    # lora rank here, we will calculate corresponding $\\hat{r}$ in MoRA\n",
    "    r=args[\"rank\"],\n",
    "    # MoRA does not use lora_alpha\n",
    "    # lora_alpha=lora_alpha,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=args[\"lora_dropout\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # **kwargs,\n",
    ")\n",
    "\n",
    "small_model = GPTNeoXForCausalLM.from_pretrained(args[\"small_model\"])\n",
    "small_model = get_peft_model(small_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1024+3072)*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 180*180\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 1024)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=180, out_features=180, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=180, out_features=180, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=1024, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTNeoXForCausalLM(\n",
       "      (gpt_neox): GPTNeoXModel(\n",
       "        (embed_in): Embedding(50304, 512)\n",
       "        (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x GPTNeoXLayer(\n",
       "            (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (attention): GPTNeoXAttention(\n",
       "              (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "              (query_key_value): lora.Linear(\n",
       "                (base_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (mlp): GPTNeoXMLP(\n",
       "              (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (act): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_mora_weights(old_module, new_rank):\n",
    "\n",
    "    new_module = nn.Linear(new_rank, new_rank, bias=old_module.bias is not None)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        new_module.weight.copy_(old_module.weight[:new_rank, :new_rank])\n",
    "\n",
    "        if old_module.bias is not None:\n",
    "            new_module.bias.copy_(old_module.bias[:new_rank])\n",
    "\n",
    "    return new_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on the large model\n",
    "\n",
    "eval_loss, eval_rouge_scores = evaluate_model(large_model, args[\"eval_dataloader\"], args[\"accelerator\"], tokenizer)\n",
    "\n",
    "# Save the evaluation results to eval_results\n",
    "eval_results = eval_results.append(\n",
    "    {\n",
    "        \"model\": \"fine_tuned_\" + args[\"large_model\"],\n",
    "        \"rank\": args[\"rank\"],\n",
    "        \"eval_loss\": eval_loss,\n",
    "        **eval_rouge_scores,\n",
    "    },\n",
    "    ignore_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the small model (before fine-tuning)\n",
    "\n",
    "eval_loss, eval_rouge_scores = evaluate_model(small_model, args[\"eval_dataloader\"], args[\"accelerator\"], tokenizer)\n",
    "\n",
    "# Save the evaluation results to eval_results\n",
    "eval_results = eval_results.append(\n",
    "    {\n",
    "        \"model\": \"raw_\" + args[\"small_model\"],\n",
    "        \"rank\": args[\"rank\"],\n",
    "        \"eval_loss\": eval_loss,\n",
    "        **eval_rouge_scores,\n",
    "    },\n",
    "    ignore_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rhat = 128\n",
    "\n",
    "for name, module in large_model.named_modules():\n",
    "    if name.endswith(\"lora_A\") or name.endswith(\"lora_B\"):\n",
    "        new_module = truncate_mora_weights(module.default, new_rhat)\n",
    "        parts = name.split('.')\n",
    "        parent_module = small_model\n",
    "        for part in parts[:-1]:\n",
    "\n",
    "            if hasattr(parent_module, part):\n",
    "                parent_module = getattr(parent_module, part)\n",
    "        \n",
    "        setattr(parent_module, parts[-1], nn.ModuleDict({\"default\": new_module}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the small model (after copying the weights)\n",
    "\n",
    "eval_loss, eval_rouge_scores = evaluate_model(small_model, args[\"eval_dataloader\"], args[\"accelerator\"], tokenizer)\n",
    "\n",
    "# Save the evaluation results to eval_results\n",
    "eval_results = eval_results.append(\n",
    "    {\n",
    "        \"model\": \"truncated_\" + args[\"small_model\"] + \"_from_\" + args[\"large_model\"],\n",
    "        \"rank\": args[\"rank\"],\n",
    "        \"eval_loss\": eval_loss,\n",
    "        **eval_rouge_scores,\n",
    "    },\n",
    "    ignore_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the evaluation results to a CSV file\n",
    "eval_results.to_csv(\"eval_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
